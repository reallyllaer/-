{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver # 브라우저를 조작하는 모듈\n",
    "from selenium.webdriver.chrome.service import Service # 크롬 드라이버의 시작과 중지를 담당하는 클래스\n",
    "from selenium.webdriver.common.by import By # 엘리먼트를 어떤 방식으로 선택할 지에 대한 상수\n",
    "from webdriver_manager.chrome import ChromeDriverManager # 크롬 드라이버를 관리\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_review(soup): # 모든 리뷰 갯수 찾아오는 함수\n",
    "    review = soup.find('span', attrs={'class':'_3HJHJjSrNK'}).text # soup에서 찾아라. <span class=\"_3HJHJjSrNK\">20,813</span>를 여기서 text(20,813)만 가져와라\n",
    "    review = int(review.replace(',', '')) # ,날리고 정수만 남겨서 저장\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_per_page(soup): # 하나의 페이지에 있는 리뷰(20개) 찾아오는 함수\n",
    "    content= soup.find_all('div', attrs={'class':'YEtwtZFLDz'}) # find_all을 씀으로써 순서대로 20개의 댓글 찾아옴\n",
    "    return len(content) # 갯수 반환(마지막 페이지에서만 갯수가 다를 수 있음!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_click(driver, i): \n",
    "    driver.find_element(By.CSS_SELECTOR, f'#REVIEW > div > div._180GG7_7yx > div.cv6id6JEkg > div > div > a:nth-child({i})').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_click(driver): # 리뷰 클릭 동작하는 함수\n",
    "    selector = \"a[href='#'][class='_11xjFby3Le N=a:tab.review']\" # 리뷰 버튼 위치를 selector로 지정\n",
    "    button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector)))\n",
    "    button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collect(soup, i):\n",
    "    # 댓글 본문 수집\n",
    "    content= soup.find_all('div', attrs={'class':'YEtwtZFLDz'})[i].text\n",
    "    content = content.replace('\\n', ' ')\n",
    "    \n",
    "    # 평점 수집\n",
    "    score = soup.select('div._1YShY6EQ56 > div > div > div > em._15NU42F3kT')[i].text\n",
    "    score = int(score)\n",
    "    \n",
    "    # 날짜 수집\n",
    "    date = soup.select('div._2FmJXrTVEX > span._3QDEeS6NLn')[i].text\n",
    "    \n",
    "    # 옵션 수집\n",
    "    option = soup.select('div._14FigHP3K8')[i] # 옵션 + 평가(=특징)까지 같이 포함되어 있는 경로\n",
    "    # extract이 pop함수와 비슷하다고 생각하시면 편합니다. 하위 태그인 dl을 반환과 동시에 삭제하는 함수입니다.\n",
    "    try:\n",
    "        satis = option.find('dl').extract()\n",
    "    except:\n",
    "        pass\n",
    "    option = option.text\n",
    "    \n",
    "    # 평가 수집\n",
    "    satis_list = []\n",
    "    try:\n",
    "        for j in range(len(satis.select('dt'))): # satis에 들어간 태그 안에 dt(=평가(헤어,세정력,두피 등))가 몇 개 있는지\n",
    "            text = satis.select('dt')[j].text +'_'+ satis.select('dd')[j].text\n",
    "            satis_list.append(text)\n",
    "    except:\n",
    "        text = ''\n",
    "        satis_list.append(text)\n",
    "    data = [content, score, date, option, satis_list]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = ['https://brand.naver.com/drbanggiwon/products/3327988855?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7485fddecb5f4bbda1e579d10992e680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "outer:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e1d7c0cee842c2a6cb497a0409e2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "inner:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--start-maximized') # 전체 화면\n",
    "options.add_argument(\"--incognito\") # 시크릿 모드\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=options\n",
    ")\n",
    "\n",
    "for url in tqdm(url_list, desc=\"outer\", position=0):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href='#'][class='_11xjFby3Le N=a:tab.review']\")))\n",
    "    # WebDriverWait를 통해 driver를 최대 10초동안 기다린다.\n",
    "    # 언제까지? -> ID가 \"a[href='#'][class='_11xjFby3Le N=a:tab.review']\" 인 엘리먼트가 나올 때까지\n",
    "    review_click(driver) # 리뷰 버튼 클릭(이 버튼을 눌러야 리뷰들이 로딩됨)\n",
    "    driver.find_element(By.CSS_SELECTOR, 'body').send_keys(Keys.PAGE_DOWN)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.YEtwtZFLDz\")))    \n",
    "    # WebDriverWait를 통해 driver를 최대 10초동안 기다린다.    \n",
    "    # 언제까지? -> ID가 \"div.YEtwtZFLDz\" 인 엘리먼트가 나올때까지(해당 엘리먼트는 '댓글'의 위치. 즉, 한 페이지에 20개의 동일한 엘리먼트가 존재)\n",
    "    html = driver.page_source # 위의 작업이 수행된 후 해당 접속 사이트 url 정보 가져옴(html로 변수 지정)\n",
    "    soup = BeautifulSoup(html, 'lxml') # html(변수)를 통해 가져온 HTML 문서를 lxml 파서를 통해서 BeautifulSoup 객체로 만들어줌\n",
    "    \n",
    "    review = all_review(soup)\n",
    "    \n",
    "    page_num = (review // 20) + 1\n",
    "    result = []\n",
    "    # 1페이지 수집\n",
    "    for i in range(review_per_page(soup)):\n",
    "        data = data_collect(soup, i)\n",
    "        result.append(data)\n",
    "\n",
    "    # 2페이지 부터 수집\n",
    "    for _ in tqdm(range(page_num//10 + 1), desc='inner', position=1):\n",
    "        for i in range(3, 13): # 2페이지부터 11페이지 \n",
    "            try:\n",
    "                next_click(driver, i)\n",
    "            except:\n",
    "                break\n",
    "            time.sleep(0.2)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            \n",
    "            for j in range(review_per_page(soup)):\n",
    "                data = data_collect(soup, j)\n",
    "                result.append(data)\n",
    "                \n",
    "            time.sleep(0.3)\n",
    "\n",
    "    results_df = pd.DataFrame(result)\n",
    "    results_df.columns = ['content', 'score', 'date', 'option', 'satis_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('C:/Users/csjin/Desktop/crawling/data/닥터기방원_최성진.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
